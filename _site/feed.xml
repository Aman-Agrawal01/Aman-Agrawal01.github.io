<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2022-02-10T23:19:16-08:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Aman Agrawal</title><subtitle>personal description</subtitle><author><name>Aman Agrawal</name><email>amanagr@iitk.ac.in</email></author><entry><title type="html">Generative Adversarial Networks</title><link href="http://localhost:4000/posts/2022/2/gans/" rel="alternate" type="text/html" title="Generative Adversarial Networks" /><published>2022-02-11T00:00:00-08:00</published><updated>2022-02-11T00:00:00-08:00</updated><id>http://localhost:4000/posts/2022/2/blog</id><content type="html" xml:base="http://localhost:4000/posts/2022/2/gans/">&lt;p&gt;Generative Adversarial Networks or in short GAN’s have the most interesting and appealing applications in my opinion ranging from generating new samples for data augmentation to create deepfakes or transforming images from one domain to the other domain. So, I thought of writing about this and appreciate the math behind it.&lt;/p&gt;

&lt;p&gt;This blog requires the basic understanding of Probability and Statistics and Neural Nets.&lt;/p&gt;

&lt;h1 id=&quot;problem-statement&quot;&gt;Problem Statement&lt;/h1&gt;

&lt;p&gt;Suppose, we are given the samples from a distribution and distribution is unknown to us. These samples could be anything from images to audio. Our aim is to generate more samples that looks like they came from the same distribution. In order to do so, we try to model a distribution that approximates the target distribution.&lt;/p&gt;

&lt;h1 id=&quot;basics&quot;&gt;Basics&lt;/h1&gt;

&lt;p&gt;Adversarial refers to conflict/opposition. So, GANs consists of ‘adversarial’ training between two parties (in our case, the parties would be neural nets) such that we will be able to ‘generate’ new samples from the data. Those parties are Generator and Discriminator.&lt;/p&gt;

&lt;p&gt;The work of the Generator is to transform a easy to sample distribution (like a D-dimensional Normal Distribution etc.) to a probability distribution that we want as close as the target distribution. Discriminator is a Binary Classifier which gives the probability whether the sample came from the data. So, both these parties conflict with each other in such a way so that the Generator tries to generate more realistic samples by the feedback that Discriminator gives on the samples.&lt;/p&gt;

&lt;p&gt;So, this is some kind of Police-Scammer case, where Scammer (Generator) tries to generate fake currency and Police (Discriminator) distinguishes fake and real currency and we want Scammer to generate realistic currency such that the Police gets confused between the real and fake. So, we can see some kind of adversary.&lt;/p&gt;

&lt;p&gt;Why do we want both these parties to get trained? It’s because we neither want the tough Discriminator which won’t help Generator to progress nor a poor Discriminator by which Generator won’t produce realistic samples.&lt;/p&gt;

&lt;h1 id=&quot;notations&quot;&gt;Notations&lt;/h1&gt;

&lt;p&gt;Say the data is denoted as $x$. The data underlies under the distribution $p_d$. Let the easy to sample distribution be denoted as $p_z$, distribution transformed be denoted as $p_g$ and the parameters of Generator $G$ and Discriminator $D$ denoted by \(\theta_g\) and $\theta_d$ respectively. The loss function is denoted as $\mathcal{L}$.&lt;/p&gt;

&lt;h1 id=&quot;loss-function&quot;&gt;Loss function&lt;/h1&gt;

&lt;p&gt;Let’s use BCE (Binary Cross-Entropy) loss in this case.&lt;/p&gt;

\[\begin{equation*}
\mathcal{L}(\theta_g,\theta_d) = \mathbb{E}_{x \sim p_d(x)}[-log(D(x))]+\mathbb{E}_{x \sim p_g(x)}[-log(1-D(x))]\
\end{equation*}\]

\[\begin{equation*}
 = \mathbb{E}_{x \sim p_d(x)}[-log(D(x))]+\mathbb{E}_{z \sim p_Z(z)}[-log(1-D(G(z)))]\
\end{equation*}\]

&lt;p&gt;The Generator $G$ and Discriminator $D$ play the two-player mini-max game with the loss function $\mathcal{L}$. Generator wants $D(G(z))$ to be close to 1 while Discriminator $D$ wants it to close to 0 and $D(x)$ close to 1. So, $G$ wants to maximise the $\mathcal{L}$ while $D$ wants to minimise $\mathcal{L}$.&lt;/p&gt;

\[\begin{equation*}
    \max_{G} \min_{D} \mathcal{L}(\theta_g,\theta_d)
\end{equation*}\]

&lt;h1 id=&quot;analysis&quot;&gt;Analysis&lt;/h1&gt;

&lt;p&gt;Let’s see why this works. First, let’s check what is the optimal discriminator for us. Fix teh Generator $G$.&lt;/p&gt;

&lt;p&gt;\(\begin{equation*}
  \mathcal{L}(\theta_g,\theta_d) = \mathbb{E}_{x \sim p_d(x)}[-log(D(x))]+\mathbb{E}_{x \sim p_g(x)}[-log(1-D(x))]
\end{equation*}\)
\(\begin{equation*}
  = \int_x p_d(x)(-log(D(x))) + p_g(x)(-log(1-D(x))) \,dx 
\end{equation*}\)&lt;/p&gt;

&lt;p&gt;Now, differentiating this integral with respect to $D(x)$ and equating with zero gives the optimal Discriminator $D^\ast(x)$. (Exercise for the reader). Assume the paramters of $D^\ast(x)$ be $\theta^\ast_d$.&lt;/p&gt;

\[\begin{equation*}
  D^\ast(x) = \frac{p_d(x)}{p_d(x)+p_g(x)}
\end{equation*}\]

&lt;p&gt;Now, let’s look at the Generator part.&lt;/p&gt;

&lt;p&gt;\(\begin{equation*}
  \mathcal{L}(\theta_g,\theta^\ast_d) = \mathbb{E}_{x \sim p_d(x)}[-log(D^\ast(x))]+\mathbb{E}_{x \sim p_g(x)}[-log(1-D^\ast(x))]
\end{equation*}\)
\(\begin{equation*}
  = \mathbb{E}_{x \sim p_d(x)}[-log(p_d(x))+log(p_d(x)+p_g(x))]+\mathbb{E}_{x \sim p_g(x)}[-log(p_g(x))+log(p_d(x)+p_g(x))]
\end{equation*}\)
\(\begin{equation*}
  = 2*log(2) - D_{KL}(p_d||\frac{p_d+p_g}{2}) - D_{KL}(p_g||\frac{p_d+p_g}{2})
\end{equation*}\)
\(\begin{equation*}
  = 2*log(2) - 2*JSD(p_d||p_g)
\end{equation*}\)&lt;/p&gt;

&lt;p&gt;where $D_{KL}$ and $JSD$ are KL Divergence and Jenson Shannon Divergence respectively. So, $JSD$ is a non-negative quantity and attains zero when both the distributions are equal. Since, the Generator $G$ wants to maximise the loss function hence the global optimum will attain when $p_g = p_d$ and that’s what we wanted.&lt;/p&gt;

&lt;h1 id=&quot;drawback&quot;&gt;Drawback&lt;/h1&gt;

&lt;p&gt;The drawback of GANs is sometimes the Generator is able to generate only handful of samples and not able to produce variety of samples. This from of failure is known as Mode Collapse.&lt;/p&gt;</content><author><name>Aman Agrawal</name><email>amanagr@iitk.ac.in</email></author><category term="ML" /><summary type="html">Generative Adversarial Networks or in short GAN’s have the most interesting and appealing applications in my opinion ranging from generating new samples for data augmentation to create deepfakes or transforming images from one domain to the other domain. So, I thought of writing about this and appreciate the math behind it.</summary></entry><entry><title type="html">Quantum Query Complexity</title><link href="http://localhost:4000/posts/2021/11/query_complexity/" rel="alternate" type="text/html" title="Quantum Query Complexity" /><published>2021-12-02T00:00:00-08:00</published><updated>2021-12-02T00:00:00-08:00</updated><id>http://localhost:4000/posts/2021/11/blog</id><content type="html" xml:base="http://localhost:4000/posts/2021/11/query_complexity/">&lt;p&gt;My first official blog! 
I have been learning query complexity and thought of writing it in a blog. This blog requires some basic understanding of quantum computing specifically the postulates of quantum mechanics.&lt;/p&gt;

&lt;h1 id=&quot;query-model&quot;&gt;Query Model&lt;/h1&gt;

&lt;p&gt;In Query Model, our task is to compute the function $f(x_1,x_2,….,x_n)$ by accessing $x_i$ through quering the orcale. Oracles can be thought of as a ‘black-box’ by which you can access some information regarding the input, we don’t really care about the internal setup in black-box, it can be easily constructed. To access input, algorithm is allowed to query the oracle to get information on input. Hence, the aim of the algorithm is to take minimum queries to compute $f(x)$ on any input $x$.&lt;/p&gt;

&lt;p&gt;A quantum Oracle $O_x$ takes qubit $\lvert i \rangle$ where $i\in[n]$ in binary representation and $n\in \mathbb{N}$ and ancilia qubit $\lvert b \rangle$ and gives $\lvert i \rangle$ and $\lvert b \oplus x_i \rangle$ where $x_i$ is the $i^{th}$ bit of input $x$.&lt;/p&gt;

&lt;p&gt;$\begin{gather} O_x \lvert i \rangle \lvert b \rangle = \lvert i \rangle \lvert b \oplus x_i \rangle = (-1)^{b*x_i} \lvert i \rangle \lvert b \rangle\ \end{gather}$&lt;/p&gt;

&lt;p&gt;This type of oracle is known as bit-flip oracle. It’s simply a unitary reversible mapping of $ (i,b) \mapsto (i, b \oplus x_i)$. Such type of mapping is used beacuse the operations we do in quantum computation should be unitary. An interesting fact, you can easily get back to $(i,b)$ from $(i,b \oplus x_i)$ by simply applying the same oracle $O_x$ to $(i,b \oplus x_i)$ due to the fact that $x_i \oplus x_i = 0$ for any $x_i$.&lt;/p&gt;

&lt;p&gt;I wanted to talk about one more oracle known as phase oracle which is widely used in quantum algorithms. You can take this as a special case of bit flip oracle where ancilia qubit is $\lvert - \rangle$ . The qubit $\lvert - \rangle$ is simply $\frac{\lvert 0 \rangle - \lvert 1 \rangle}{\sqrt{2}}$ . If we apply the bit flip oracle in this case, we can observe that the output would just changes the phase of our original state.&lt;/p&gt;

&lt;p&gt;$\begin{gather} O_x \lvert i \rangle \lvert - \rangle = (-1)^{x_i} \lvert i \rangle \lvert - \rangle \ \end{gather}$&lt;/p&gt;

&lt;h1 id=&quot;query-complexity&quot;&gt;Query Complexity&lt;/h1&gt;

&lt;p&gt;The query complexity is used in deterministic as well as quantum algorithms. One should note that there is a difference between query complexity of an algorithm and that of a function.&lt;/p&gt;

&lt;p&gt;The query complexity of an algorithm would be the minimum number of queries that it takes to compute the function $f(x)$ on any input $x$.&lt;/p&gt;

&lt;p&gt;The query complexity of a function is the minimum number of queries that an optimal algorithm takes in order to compute function $f(x)$ on any input $x$.&lt;/p&gt;

&lt;h1 id=&quot;why-query-model&quot;&gt;Why Query Model?&lt;/h1&gt;

&lt;p&gt;Now you might ask, why do we really need to know about query complexity? That’s a valid question to ask. There are couple of reasons that convince us to have an idea about query complexity.&lt;/p&gt;

&lt;p&gt;Firstly, it’s easy to analyse and a lot of known quantum algorithms like Shor’s algorithm and Grover’s search are based on query model.&lt;/p&gt;

&lt;p&gt;Secondly, it gives us a bound on time complexity of an algorithm. Finding the time complexity of a quantum algorithm is a tricky task but it turns out that if we are able to find the query complexity of the algorithm than we know that time complexity is larger than query complexity and hence gets some idea about the time complexity of an algorithm.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;So, that’s it! Hope you like the blog :) I am planning to write one more blog on a result “the approximate degree gives a lower bound on quantum query complexity”.&lt;/p&gt;</content><author><name>Aman Agrawal</name><email>amanagr@iitk.ac.in</email></author><category term="quantum" /><category term="complexity" /><summary type="html">My first official blog! I have been learning query complexity and thought of writing it in a blog. This blog requires some basic understanding of quantum computing specifically the postulates of quantum mechanics.</summary></entry><entry><title type="html">Hello World</title><link href="http://localhost:4000/posts/2021/10/hello-world/" rel="alternate" type="text/html" title="Hello World" /><published>2021-10-11T00:00:00-07:00</published><updated>2021-10-11T00:00:00-07:00</updated><id>http://localhost:4000/posts/2021/10/blog-post-1</id><content type="html" xml:base="http://localhost:4000/posts/2021/10/hello-world/">&lt;p&gt;My first blog! Stay tuned for more.&lt;/p&gt;</content><author><name>Aman Agrawal</name><email>amanagr@iitk.ac.in</email></author><category term="cool posts" /><summary type="html">My first blog! Stay tuned for more.</summary></entry></feed>