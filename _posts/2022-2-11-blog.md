---
title: 'Generative Adversarial Networks'
date: 2022-2-11
permalink: /posts/2022/2/gans/
tags:
  - ML
---

Generative Adversarial Networks or in short GAN's have the most interesting and appealing applications in my opinion ranging from generating new samples for data augmentation to create deepfakes or transforming images from one domain to the other domain. So, I thought of writing about this and appreciate the math behind it.

This blog requires the basic understanding of Probability and Statistics and Neural Nets. 

Problem Statement
===

Suppose, we are given the samples from a distribution and distribution is unknown to us. These samples could be anything from images to audio. Our aim is to generate more samples that looks like they came from the same distribution. In order to do so, we try to model a distribution that approximates the target distribution.

Basics
=== 

Adversarial refers to conflict/opposition. So, GANs consists of 'adversarial' training between two parties (in our case, the parties would be neural nets) such that we will be able to 'generate' new samples from the data. Those parties are Generator and Discriminator. 

The work of the Generator is to transform a easy to sample distribution (like a D-dimensional Normal Distribution etc.) to a probability distribution that we want as close as the target distribution. Discriminator is a Binary Classifier which gives the probability whether the sample came from the data. So, both these parties conflict with each other in such a way so that the Generator tries to generate more realistic samples by the feedback that Discriminator gives on the samples. 

So, this is some kind of Police-Scammer case, where Scammer (Generator) tries to generate fake currency and Police (Discriminator) distinguishes fake and real currency and we want Scammer to generate realistic currency such that the Police gets confused between the real and fake. So, we can see some kind of adversary.  

Why do we want both these parties to get trained? It's because we neither want the tough Discriminator which won't help Generator to progress nor a poor Discriminator by which Generator won't produce realistic samples. 

Notations
=== 

Say the data is denoted as $x$. The data underlies under the distribution $p_d$. Let the easy to sample distribution be denoted as $p_z$, distribution transformed be denoted as $p_g$ and the parameters of Generator $G$ and Discriminator $D$ denoted by $$\theta_g$$ and $\theta_d$ respectively. The loss function is denoted as $\mathcal{L}$.

Loss function 
=== 

Let's use BCE (Binary Cross-Entropy) loss in this case. 

$$ 
\begin{equation*}
\mathcal{L}(\theta_g,\theta_d) = \mathbb{E}_{x \sim p_d(x)}[-log(D(x))]+\mathbb{E}_{x \sim p_g(x)}[-log(1-D(x))]\
\end{equation*}
$$

$$ 
\begin{equation*}
 = \mathbb{E}_{x \sim p_d(x)}[-log(D(x))]+\mathbb{E}_{z \sim p_Z(z)}[-log(1-D(G(z)))]\
\end{equation*}
$$
